{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuUEjAwP1OQc"
      },
      "source": [
        "## Imports and configs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ITOAckJ7RJYC"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, AutoConfig\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lx_rAHzX2bmq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training on cuda\n"
          ]
        }
      ],
      "source": [
        "from torch import cuda\n",
        "\n",
        "config = {'model_name': 'google/bigbird-roberta-base',\n",
        "          'max_length': 1024,\n",
        "          'train_batch_size': 4,\n",
        "          'valid_batch_size': 4,\n",
        "          'epochs':5,\n",
        "          'learning_rates': [2.5e-5, 2.5e-5, 2.5e-6, 2.5e-6, 2.5e-7],\n",
        "          'max_grad_norm': 10,\n",
        "          'device': 'cuda' if cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'} # Added support for Apple Metal acceleration.\n",
        "\n",
        "print(f\"Training on {config['device']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "bxk_MhBgwk5x",
        "outputId": "cd65b40d-8d07-4ef9-ed98-6f97ee280cba"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>content</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>73DC1D49FAD5</td>\n",
              "      <td>eletoral college can be a very good thing caus...</td>\n",
              "      <td>[ 3  4  4  4  4  4  4  4  4  4  4  4  4  4  4 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>D840AC3957E5</td>\n",
              "      <td>STUDENT_NAME\\n\\nADDRESS_NAME\\n\\nFebruary 22, 2...</td>\n",
              "      <td>[ 0  0  0  0  0  0  0  3  4  4  4  4  4  4  4 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>753E320B186B</td>\n",
              "      <td>In my opinion as a student: I don't agree at t...</td>\n",
              "      <td>[1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>C2ABDAC2BC2C</td>\n",
              "      <td>When it comes to at home learning and attendin...</td>\n",
              "      <td>[ 3  4  4  4  4  4  4  4  4  4  4  4  4  4  4 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>B2DDBAAC084C</td>\n",
              "      <td>Y\\n\\nou can ask many different people for advi...</td>\n",
              "      <td>[ 3  4  4  4  4  4  4  4  4  4  4  4  4  4  4 ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             id                                            content  \\\n",
              "0  73DC1D49FAD5  eletoral college can be a very good thing caus...   \n",
              "1  D840AC3957E5  STUDENT_NAME\\n\\nADDRESS_NAME\\n\\nFebruary 22, 2...   \n",
              "2  753E320B186B  In my opinion as a student: I don't agree at t...   \n",
              "3  C2ABDAC2BC2C  When it comes to at home learning and attendin...   \n",
              "4  B2DDBAAC084C  Y\\n\\nou can ask many different people for advi...   \n",
              "\n",
              "                                              labels  \n",
              "0  [ 3  4  4  4  4  4  4  4  4  4  4  4  4  4  4 ...  \n",
              "1  [ 0  0  0  0  0  0  0  3  4  4  4  4  4  4  4 ...  \n",
              "2  [1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2...  \n",
              "3  [ 3  4  4  4  4  4  4  4  4  4  4  4  4  4  4 ...  \n",
              "4  [ 3  4  4  4  4  4  4  4  4  4  4  4  4  4  4 ...  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "full_df = pd.read_csv('../dataset.csv')\n",
        "full_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Labels\n",
        "\n",
        "Because we have 15k+ essays, each with hundreds of labeled words, we store the labels as integers rather than strings to drastically decrease disk and memory usage (see `preprocessing.ipynb` for how this was done). Additionally, we would have needed to do this for training regardless to build the one-hot arrays. These can be easily converted to/from their original string labels with the dictionaries below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "sKy9ejb77YQ6"
      },
      "outputs": [],
      "source": [
        "id2label = {\n",
        "  0:  'Unnanotated',\n",
        "  1:  'B-Lead',\n",
        "  2:  'I-Lead',\n",
        "  3:  'B-Position',\n",
        "  4:  'I-Position',\n",
        "  5:  'B-Evidence',\n",
        "  6:  'I-Evidence',\n",
        "  7:  'B-Claim',\n",
        "  8:  'I-Claim',\n",
        "  9:  'B-Concluding_Statement',\n",
        "  10: 'I-Concluding_Statement',\n",
        "  11: 'B-Counterclaim',\n",
        "  12: 'I-Counterclaim',\n",
        "  13: 'B-Rebuttal',\n",
        "  14: 'I-Rebuttal'\n",
        "}\n",
        "\n",
        "label2id = {\n",
        "  'Unnanotated': 0,\n",
        "  'B-Lead': 1,\n",
        "  'I-Lead': 2,\n",
        "  'B-Position': 3,\n",
        "  'I-Position': 4,\n",
        "  'B-Evidence': 5,\n",
        "  'I-Evidence': 6,\n",
        "  'B-Claim': 7,\n",
        "  'I-Claim': 8,\n",
        "  'B-Concluding_Statement': 9,\n",
        "  'I-Concluding_Statement': 10,\n",
        "  'B-Counterclaim': 11,\n",
        "  'I-Counterclaim': 12,\n",
        "  'B-Rebuttal': 13,\n",
        "  'I-Rebuttal': 14\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset\n",
        "\n",
        "Since we're using the PyTorch backend, it is convenient to define the torch Dataset (and later the Dataloader), so that the model can easily intake the data without any dependencies on how the we chose to store the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "SjLfEutKxepV"
      },
      "outputs": [],
      "source": [
        "class EssayDataset(Dataset):\n",
        "  def __init__(self, df, tokenizer, max_len, get_word_ids):\n",
        "        self.len = len(df)\n",
        "        self.df = df\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.get_word_ids = get_word_ids # For validation.\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "        essay_words = self.df.content[index].split() # Split essay by words before tokenizing.\n",
        "\n",
        "        encoding = self.tokenizer(essay_words,\n",
        "                             is_split_into_words=True, # Necessary to keep correspondance between words and labels contructed previously.\n",
        "                             padding='max_length',\n",
        "                             truncation=True,\n",
        "                             max_length=self.max_len)\n",
        "        \"\"\"\n",
        "        From Tokenizer's docs about word_ids:\n",
        "            A list indicating the word corresponding to each token. Special tokens added by the tokenizer are mapped to None and other tokens\n",
        "            are mapped to the index of their corresponding word (several tokens will be mapped to the same word index if they are parts of that word).\n",
        "\n",
        "        This is needed to match the correct labels with the tokens, which may not have a 1:1 correspondence with the original words.\n",
        "        \"\"\"\n",
        "        word_ids = encoding.word_ids()\n",
        "        word_labels = None\n",
        "\n",
        "        # If we're training, we want to know the labels corresponding to each word_id.\n",
        "        if not self.get_word_ids:\n",
        "            # Get original word label array for this essay.\n",
        "            word_labels = [int(label) for label in self.df.labels[index].split()]\n",
        "            label_ids = []\n",
        "\n",
        "            # Correct for tokenization mismatch.\n",
        "            for word_idx in word_ids:\n",
        "                # 'None' means that this is a special/reserved token, mark as -100 to be ignored later in training.\n",
        "                if word_idx is None:\n",
        "                    label_ids.append(-100) # Magic number, automatically ignored by CrossEntropyLoss.\n",
        "                else:\n",
        "                    label_ids.append(word_labels[word_idx])\n",
        "\n",
        "            encoding['labels'] = label_ids\n",
        "\n",
        "        # Otherwise, it does not matter since we are predicting the labels, and we jus need to know the token-id correspondence for label attribution.\n",
        "        else:\n",
        "            word_ids2 = [w if w is not None else -1 for w in word_ids]\n",
        "            encoding['word_ids'] = torch.as_tensor(word_ids2)\n",
        "\n",
        "        item = {k: torch.as_tensor(v) for k, v in encoding.items()}\n",
        "\n",
        "        # if self.get_word_ids:\n",
        "        #     word_ids2 = [w if w is not None else -1 for w in word_ids]\n",
        "        #     item['word_ids'] = torch.as_tensor(word_ids2)\n",
        "\n",
        "        return item\n",
        "\n",
        "  def __len__(self):\n",
        "        return self.len"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOJU-81Jv2xO"
      },
      "source": [
        "### Create training and validation datasets + DataLoaders\n",
        "\n",
        "We will use a 85%/15% split between training and validation sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "b4VZ2KiU0AHt"
      },
      "outputs": [],
      "source": [
        "validation_split_size = 0.15\n",
        "dataset_split_seed = 33 # Any seed, here for inter-run consistency."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNmagNKo1E9H"
      },
      "source": [
        "#### Split dataset into training and validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n82RG8PQv8Ph",
        "outputId": "c5fdbbea-2809-42b4-e4cd-8798fc651deb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train_df shape:  (13254, 2)\n",
            "val_df shape:  (2340, 3)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_df, val_df = train_test_split(full_df, test_size=validation_split_size, random_state=dataset_split_seed)\n",
        "\n",
        "# Drop id column from train dataframe\n",
        "train_df = train_df[['content', 'labels']]\n",
        "\n",
        "train_df.reset_index(drop=True, inplace=True)\n",
        "val_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "print(\"train_df shape: \", train_df.shape)\n",
        "print(\"val_df shape: \", val_df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKYQoL-A18pu"
      },
      "source": [
        "#### Download/cache all models necessary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "N1XQ6jVOG0iW"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BigBirdForTokenClassification were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "model_save_path = './model/'\n",
        "\n",
        "if not os.path.exists(model_save_path):\n",
        "    os.mkdir(model_save_path)\n",
        "\n",
        "model_name = 'google/bigbird-roberta-base' # From Huggingface's ModelHub.\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True, id2label=id2label)\n",
        "tokenizer.save_pretrained(model_save_path)\n",
        "\n",
        "config_model = AutoConfig.from_pretrained(model_name) \n",
        "config_model.num_labels = len(label2id)\n",
        "config_model.save_pretrained(model_save_path)\n",
        "\n",
        "backbone = AutoModelForTokenClassification.from_pretrained(model_name, \n",
        "                                                           config=config_model)\n",
        "backbone.save_pretrained(model_save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Initialize Dataset and tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxxF_cKHtC70",
        "outputId": "05bcee71-780e-4eca-9b56-5f12a75eabb0"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('./model')\n",
        "training_set = EssayDataset(df=train_df, tokenizer=tokenizer, max_len=config['max_length'], get_word_ids=False)\n",
        "testing_set = EssayDataset(df=val_df, tokenizer=tokenizer, max_len=config['max_length'], get_word_ids=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "IgaOgMmW2J46"
      },
      "outputs": [],
      "source": [
        "train_params = {'batch_size': config['train_batch_size'],\n",
        "                'shuffle': True,\n",
        "                'num_workers': 2,\n",
        "                }\n",
        "\n",
        "validation_params = {'batch_size': config['valid_batch_size'],\n",
        "               'shuffle': False,\n",
        "               'num_workers': 2,\n",
        "               }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Instantiate Dataloader (from torch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "lRDZGjt52KcU"
      },
      "outputs": [],
      "source": [
        "training_loader = DataLoader(training_set, **train_params)\n",
        "validation_loader = DataLoader(testing_set, **validation_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2ieuYIrIDSO"
      },
      "source": [
        "Save DataLoaders to be able to quickly load them later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "GEcuk2cjHMCV"
      },
      "outputs": [],
      "source": [
        "# torch.save(training_loader, 'training_loader.pth')\n",
        "# torch.save(validation_loader, 'validation_loader.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
