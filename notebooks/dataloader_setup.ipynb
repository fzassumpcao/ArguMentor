{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuUEjAwP1OQc"
      },
      "source": [
        "## Imports and configs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ITOAckJ7RJYC"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, AutoConfig\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "lx_rAHzX2bmq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training on cuda\n"
          ]
        }
      ],
      "source": [
        "config = {'model_name': 'google/bigbird-roberta-base', # From Huggingface's ModelHub.\n",
        "          'model_save_path': './model/',\n",
        "          'model_chkpt_path': './model/model_chkpt/',\n",
        "          'max_length': 1024,\n",
        "          'train_batch_size': 4,\n",
        "          'valid_batch_size': 4,\n",
        "          'epochs':10,\n",
        "          'learning_rates': [2.5e-5, 2.5e-5, 2.5e-6, 2.5e-6, 2.5e-7],\n",
        "          'max_grad_norm': 10,\n",
        "          'device': 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'} # Added support for Apple Metal acceleration.\n",
        "\n",
        "print(f\"Training on {config['device']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "bxk_MhBgwk5x",
        "outputId": "cd65b40d-8d07-4ef9-ed98-6f97ee280cba"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>73DC1D49FAD5</th>\n",
              "      <td>eletoral college can be a very good thing caus...</td>\n",
              "      <td>[3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>D840AC3957E5</th>\n",
              "      <td>STUDENT_NAME\\n\\nADDRESS_NAME\\n\\nFebruary 22, 2...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 3, 4, 4, 4, 4, 4, 4, 4, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>753E320B186B</th>\n",
              "      <td>In my opinion as a student: I don't agree at t...</td>\n",
              "      <td>[1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C2ABDAC2BC2C</th>\n",
              "      <td>When it comes to at home learning and attendin...</td>\n",
              "      <td>[3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>B2DDBAAC084C</th>\n",
              "      <td>Y\\n\\nou can ask many different people for advi...</td>\n",
              "      <td>[3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                        content  \\\n",
              "id                                                                \n",
              "73DC1D49FAD5  eletoral college can be a very good thing caus...   \n",
              "D840AC3957E5  STUDENT_NAME\\n\\nADDRESS_NAME\\n\\nFebruary 22, 2...   \n",
              "753E320B186B  In my opinion as a student: I don't agree at t...   \n",
              "C2ABDAC2BC2C  When it comes to at home learning and attendin...   \n",
              "B2DDBAAC084C  Y\\n\\nou can ask many different people for advi...   \n",
              "\n",
              "                                                         labels  \n",
              "id                                                               \n",
              "73DC1D49FAD5  [3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...  \n",
              "D840AC3957E5  [0, 0, 0, 0, 0, 0, 0, 3, 4, 4, 4, 4, 4, 4, 4, ...  \n",
              "753E320B186B  [1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...  \n",
              "C2ABDAC2BC2C  [3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...  \n",
              "B2DDBAAC084C  [3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "full_df = pd.read_pickle('../dataset.csv')\n",
        "full_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Labels\n",
        "\n",
        "Because we have 15k+ essays, each with hundreds of labeled words, we store the labels as integers rather than strings to drastically decrease disk and memory usage (see `preprocessing.ipynb` for how this was done). Additionally, we would have needed to do this for training regardless to build the one-hot arrays. These can be easily converted to/from their original string labels with the dictionaries below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "sKy9ejb77YQ6"
      },
      "outputs": [],
      "source": [
        "id2label = {\n",
        "  0:  'Unnanotated',\n",
        "  1:  'B-Lead',\n",
        "  2:  'I-Lead',\n",
        "  3:  'B-Position',\n",
        "  4:  'I-Position',\n",
        "  5:  'B-Evidence',\n",
        "  6:  'I-Evidence',\n",
        "  7:  'B-Claim',\n",
        "  8:  'I-Claim',\n",
        "  9:  'B-Concluding_Statement',\n",
        "  10: 'I-Concluding_Statement',\n",
        "  11: 'B-Counterclaim',\n",
        "  12: 'I-Counterclaim',\n",
        "  13: 'B-Rebuttal',\n",
        "  14: 'I-Rebuttal'\n",
        "}\n",
        "\n",
        "label2id = {\n",
        "  'Unnanotated': 0,\n",
        "  'B-Lead': 1,\n",
        "  'I-Lead': 2,\n",
        "  'B-Position': 3,\n",
        "  'I-Position': 4,\n",
        "  'B-Evidence': 5,\n",
        "  'I-Evidence': 6,\n",
        "  'B-Claim': 7,\n",
        "  'I-Claim': 8,\n",
        "  'B-Concluding_Statement': 9,\n",
        "  'I-Concluding_Statement': 10,\n",
        "  'B-Counterclaim': 11,\n",
        "  'I-Counterclaim': 12,\n",
        "  'B-Rebuttal': 13,\n",
        "  'I-Rebuttal': 14\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset\n",
        "\n",
        "Since we're using the PyTorch backend, it is convenient to define the torch Dataset (and later the Dataloader), so that the model can easily intake the data without any dependencies on how the we chose to store the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "SjLfEutKxepV"
      },
      "outputs": [],
      "source": [
        "class EssayDataset(Dataset):\n",
        "  def __init__(self, df, tokenizer, max_len, get_word_ids):\n",
        "        self.len = len(df)\n",
        "        self.df = df\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.get_word_ids = get_word_ids # For validation.\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "        essay_words = self.df.content[index].split() # Split essay by words before tokenizing.\n",
        "\n",
        "        # Makes a dict with keys: input_ids, attention_mask.\n",
        "        encoding = self.tokenizer(essay_words,\n",
        "                             is_split_into_words=True, # Necessary to keep correspondance between words and labels contructed previously.\n",
        "                             padding='max_length',\n",
        "                             truncation=True,\n",
        "                             max_length=self.max_len)\n",
        "        \"\"\"\n",
        "        From Tokenizer's docs about word_ids:\n",
        "            A list indicating the word corresponding to each token. Special tokens added by the tokenizer are mapped to None and other tokens\n",
        "            are mapped to the index of their corresponding word (several tokens will be mapped to the same word index if they are parts of that word).\n",
        "\n",
        "        This is needed to match the correct labels with the tokens, which may not have a 1:1 correspondence with the original words.\n",
        "        \"\"\"\n",
        "        word_ids = encoding.word_ids()\n",
        "        word_labels = None\n",
        "\n",
        "        # If we're training, we want to know the labels corresponding to each word_id.\n",
        "        if not self.get_word_ids:\n",
        "            # Get original word label array for this essay.\n",
        "            word_labels = self.df.labels[index] #[int(label) for label in self.df.labels[index].split()]\n",
        "            label_ids = []\n",
        "\n",
        "            # Correct for tokenization mismatch.\n",
        "            for word_idx in word_ids:\n",
        "                # 'None' means that this is a special/reserved token, mark as -100 to be ignored later in training.\n",
        "                if word_idx is None:\n",
        "                    label_ids.append(-100) # Magic number, automatically ignored by CrossEntropyLoss.\n",
        "                else:\n",
        "                    label_ids.append(word_labels[word_idx])\n",
        "\n",
        "            encoding['labels'] = label_ids\n",
        "\n",
        "        # Otherwise, it does not matter since we are predicting the labels, and we jus need to know the token-id correspondence for label attribution.\n",
        "        else:\n",
        "            word_ids2 = [w if w is not None else -1 for w in word_ids]\n",
        "            encoding['word_ids'] = torch.as_tensor(word_ids2)\n",
        "\n",
        "        item = {k: torch.as_tensor(v) for k, v in encoding.items()}\n",
        "\n",
        "        # if self.get_word_ids:\n",
        "        #     word_ids2 = [w if w is not None else -1 for w in word_ids]\n",
        "        #     item['word_ids'] = torch.as_tensor(word_ids2)\n",
        "\n",
        "        return item\n",
        "\n",
        "  def __len__(self):\n",
        "        return self.len"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOJU-81Jv2xO"
      },
      "source": [
        "### Create training and validation datasets + DataLoaders\n",
        "\n",
        "We will use a 85%/15% split between training and validation sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "b4VZ2KiU0AHt"
      },
      "outputs": [],
      "source": [
        "validation_split_size = 0.15\n",
        "dataset_split_seed = 33 # Any seed, here for inter-run consistency."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNmagNKo1E9H"
      },
      "source": [
        "#### Split dataset into training and validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n82RG8PQv8Ph",
        "outputId": "c5fdbbea-2809-42b4-e4cd-8798fc651deb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train_df shape:  (13254, 2)\n",
            "val_df shape:  (2340, 2)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_df, val_df = train_test_split(full_df, test_size=validation_split_size, random_state=dataset_split_seed)\n",
        "\n",
        "# Drop id column from train dataframe\n",
        "train_df = train_df[['content', 'labels']]\n",
        "\n",
        "train_df.reset_index(drop=True, inplace=True)\n",
        "val_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "print(\"train_df shape: \", train_df.shape)\n",
        "print(\"val_df shape: \", val_df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKYQoL-A18pu"
      },
      "source": [
        "#### Download/cache all models necessary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "N1XQ6jVOG0iW"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BigBirdForTokenClassification were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "if not os.path.exists(config['model_save_path']):\n",
        "    os.mkdir(config['model_save_path'])\n",
        "\n",
        "AutoTokenizer.from_pretrained(config['model_name'], add_prefix_space=True, id2label=id2label).save_pretrained(config['model_save_path'])\n",
        "\n",
        "config_model = AutoConfig.from_pretrained(config['model_name']) \n",
        "config_model.num_labels = len(label2id)\n",
        "config_model.save_pretrained(config['model_save_path'])\n",
        "\n",
        "AutoModelForTokenClassification.from_pretrained(config['model_name'], \n",
        "                                                           config=config_model).save_pretrained(config['model_save_path'])\n",
        "\n",
        "del config_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Initialize Dataset, Dataloader, and tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "IgaOgMmW2J46"
      },
      "outputs": [],
      "source": [
        "train_params = {'batch_size': config['train_batch_size'],\n",
        "                'shuffle': True,\n",
        "                # 'num_workers': 2,\n",
        "                }\n",
        "\n",
        "validation_params = {'batch_size': config['valid_batch_size'],\n",
        "               'shuffle': False,\n",
        "            #    'num_workers': 2,\n",
        "               }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxxF_cKHtC70",
        "outputId": "05bcee71-780e-4eca-9b56-5f12a75eabb0"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(config['model_save_path'])\n",
        "\n",
        "training_set = EssayDataset(df=train_df, tokenizer=tokenizer, max_len=config['max_length'], get_word_ids=False)\n",
        "training_loader = DataLoader(training_set, **train_params)\n",
        "\n",
        "testing_set = EssayDataset(df=val_df, tokenizer=tokenizer, max_len=config['max_length'], get_word_ids=True)\n",
        "validation_loader = DataLoader(testing_set, **validation_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([-100,    1,    2,  ..., -100, -100, -100])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "training_set[0]['labels'].view(-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2ieuYIrIDSO"
      },
      "source": [
        "Save DataLoaders to be able to quickly load them later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "GEcuk2cjHMCV"
      },
      "outputs": [],
      "source": [
        "# torch.save(training_loader, 'training_loader.pth')\n",
        "# torch.save(validation_loader, 'validation_loader.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Initialize all necesary models/objects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        }
      ],
      "source": [
        "config_model = AutoConfig.from_pretrained(f\"{config['model_save_path']}config.json\") \n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "                   f\"{config['model_save_path']}model.safetensors\",config=config_model).to(config['device'])\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=config['learning_rates'][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def train_one_epoch(epoch, verbose=False):\n",
        "    # Since we're batching, and the last batch may not be the full size,\n",
        "    # keep track of precise # steps and # seen examples for metrics computations.\n",
        "    epoch_loss = 0\n",
        "    epoch_accuracy = 0\n",
        "    num_train_examples = 0\n",
        "    num_train_steps = 0\n",
        "    \n",
        "    # Set model to training mode (torch).\n",
        "    model.train()\n",
        "    \n",
        "    for idx, batch in tqdm(enumerate(training_loader), total=len(training_loader)): # TODO: Customize tqdm for better displaying.\n",
        "        \n",
        "        # The dictionary returned by trainin_loader has 3 keys: input_ids, attention_mask (both made by the Tokenizer),\n",
        "        # and labels (which we add in the Dataset abstraction, since this is a training run).\n",
        "        input_ids = batch['input_ids'].to(config['device'], dtype=torch.long)\n",
        "        attention_mask = batch['attention_mask'].to(config['device'], dtype=torch.long)\n",
        "        labels = batch['labels'].to(config['device'], dtype=torch.long)\n",
        "\n",
        "        # Run batch through model.\n",
        "        loss, train_logits = model(input_ids=input_ids,\n",
        "                                   attention_mask=attention_mask,\n",
        "                                   labels=labels,\n",
        "                                   return_dict=False)\n",
        "        \n",
        "        # Increment epoch loss by this batch's loss.\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        # Increment counters.\n",
        "        num_train_steps += 1\n",
        "        num_train_examples += labels.size(0)\n",
        "        \n",
        "        # Debugging.\n",
        "        if verbose and (idx % 100 == 0):\n",
        "            print(f\"Idx: {idx:04d}, step loss: {epoch_loss/num_train_steps}\")\n",
        "           \n",
        "        # Compute training accuracy\n",
        "        flattened_logits = train_logits.view(-1, model.num_labels) # (batch_size, sequence_length, num_labels) -> (batch_size * seq_length, num_labels)\n",
        "        flattened_predictions = torch.argmax(flattened_logits, axis=1) # Find predicted label.\n",
        "        label_mask = labels.view(-1) != -100 # If our label is -100 (as sdiscussed above), it should be ignored as it is a special token.\n",
        "        \n",
        "        # Mask both predictions and ground truth.\n",
        "        labels = torch.masked_select(input=labels.view(-1), mask=label_mask)\n",
        "        predictions = torch.masked_select(input=flattened_predictions, mask=label_mask)\n",
        "\n",
        "        # Use accuracy function from sklear.\n",
        "        epoch_accuracy += accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n",
        "    \n",
        "        # Gradient clipping.\n",
        "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=config['max_grad_norm'])\n",
        "        \n",
        "        # Finally, optimize.\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Model and dataset is pretty big, so alleviate some GPU memory usage by releasing unused cached data at every batch.\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # Normalize loss and accuracy by num steps.\n",
        "    epoch_loss /= num_train_steps\n",
        "    epoch_accuracy /= num_train_steps\n",
        "\n",
        "    print(f\"Training loss epoch: {epoch_loss}\")\n",
        "    print(f\"Training accuracy epoch: {epoch_accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Finally, train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/3314 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3314/3314 [37:39<00:00,  1.47it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss epoch: 0.765188396206338\n",
            "Training accuracy epoch: 0.7502558406782374\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3314/3314 [36:58<00:00,  1.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss epoch: 0.5831361591240406\n",
            "Training accuracy epoch: 0.7997284147194624\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3314/3314 [37:06<00:00,  1.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss epoch: 0.4913203148783944\n",
            "Training accuracy epoch: 0.8287917455815275\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3314/3314 [37:08<00:00,  1.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss epoch: 0.4055954908944248\n",
            "Training accuracy epoch: 0.8577272066393199\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3314/3314 [38:09<00:00,  1.45it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss epoch: 0.33581863592817757\n",
            "Training accuracy epoch: 0.8820413205741073\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3314/3314 [40:01<00:00,  1.38it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss epoch: 0.26749340302154073\n",
            "Training accuracy epoch: 0.9059907922725198\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3314/3314 [37:31<00:00,  1.47it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss epoch: 0.22602375974124392\n",
            "Training accuracy epoch: 0.9208606919560415\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3314/3314 [37:30<00:00,  1.47it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss epoch: 0.18980322166148733\n",
            "Training accuracy epoch: 0.9347972470033142\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3314/3314 [38:08<00:00,  1.45it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss epoch: 0.16474826169955079\n",
            "Training accuracy epoch: 0.9436050707252219\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3314/3314 [37:33<00:00,  1.47it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss epoch: 0.1467643977034956\n",
            "Training accuracy epoch: 0.950168642721515\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "from safetensors.torch import save_model\n",
        "\n",
        "if not os.path.exists(config['model_chkpt_path']):\n",
        "    os.mkdir(config['model_chkpt_path'])\n",
        "\n",
        "run_id = time.strftime(\"%Y%m%d-%H:%M:%S\")\n",
        "\n",
        "for epoch in range(config['epochs']):\n",
        "    train_one_epoch(epoch)\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    chkpt_filename = f\"{config['model_chkpt_path']}{config['model_name'].replace('/', '-')}-time={run_id}-maxlen={config['max_length']}-batchsize={config['train_batch_size']}-lr={str(config['learning_rates'])}-maxgrad={config['max_grad_norm']}-epoch={epoch}\"\n",
        "\n",
        "    # torch.save(model.state_dict(), chkpt_filename)\n",
        "    # save_model(model=model, filename=chkpt_filename, metadata=config_model)\n",
        "    model.save_pretrained(chkpt_filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'torch' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "import torch\n",
        "\n",
        "config = {'model_name': 'google/bigbird-roberta-base', # From Huggingface's ModelHub.\n",
        "          'model_save_path': './model/',\n",
        "          'model_chkpt_path': './model/model_chkpt/',\n",
        "          'max_length': 1024,\n",
        "          'train_batch_size': 4,\n",
        "          'valid_batch_size': 4,\n",
        "          'epochs':10,\n",
        "          'learning_rates': [2.5e-5, 2.5e-5, 2.5e-6, 2.5e-6, 2.5e-7],\n",
        "          'max_grad_norm': 10,\n",
        "          'device': 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'}\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(config['model_save_path'])\n",
        "model = AutoModelForTokenClassification.from_pretrained('/home/rafaelpiacsek/Documents/College/Courses/Semester_7/CS_410/FinalProject/ArguMentor/notebooks/model/model_chkpt/google-bigbird-roberta-base-time=20231214-16:41:38-maxlen=1024-batchsize=4-lr=[2.5e-05, 2.5e-05, 2.5e-06, 2.5e-06, 2.5e-07]-maxgrad=10-epoch=9').to(config['device'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "def infer(essay):\n",
        "    # Makes a dict with keys: input_ids, attention_mask.\n",
        "    encoding = tokenizer(essay.split(),\n",
        "                         is_split_into_words=True, # Necessary to keep correspondance between words and labels contructed previously.\n",
        "                         padding='max_length',\n",
        "                         truncation=True,\n",
        "                         max_length=config['max_length'])\n",
        "    \"\"\"\n",
        "    From Tokenizer's docs about word_ids:\n",
        "        A list indicating the word corresponding to each token. Special tokens added by the tokenizer are mapped to None and other tokens\n",
        "        are mapped to the index of their corresponding word (several tokens will be mapped to the same word index if they are parts of that word).\n",
        "\n",
        "    This is needed to match the correct labels with the tokens, which may not have a 1:1 correspondence with the original words.\n",
        "    \"\"\"\n",
        "    encoding['word_ids'] = torch.as_tensor([w if w is not None else -1 for w in encoding.word_ids()])\n",
        "\n",
        "    item = {k: torch.unflatten(torch.as_tensor(v), 0, (1, -1)).to(config['device']) for k, v in encoding.items()}\n",
        "\n",
        "    model.eval()\n",
        "    output_dict = model(input_ids=item['input_ids'],\n",
        "                   attention_mask=item['attention_mask'],\n",
        "                   return_dict=True)\n",
        "    # print(torch.argmax(output_dict['logits'].view(-1, model.num_labels), axis=-1).shape)\n",
        "\n",
        "    token_predictions = torch.argmax(output_dict['logits'].view(-1, model.num_labels), axis=-1)\n",
        "    \n",
        "    words_predictions = list()\n",
        "    prev_word_idx = -1\n",
        "    for idx, word_idx in enumerate(item['word_ids'][0]):\n",
        "        if word_idx == -1:\n",
        "            continue\n",
        "        elif word_idx != prev_word_idx:\n",
        "            prev_word_idx = word_idx\n",
        "            words_predictions.append(token_predictions[idx].item())\n",
        "\n",
        "    print(len(words_predictions), len(essay.split()))\n",
        "    \n",
        "    return words_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "635 635\n"
          ]
        }
      ],
      "source": [
        "test_essay = \"\"\"\n",
        "During a group project, have you ever asked a group member about adding or replacing something? Or, when you were studying for a math test, did you ever ask your parents or sibling about different ways to tackle a certain problem? Asking for other's opinions is especially beneficial as it allows for an individual to receive a variety of different views towards a given topic. Likewise, being diverse and asking many people for their opinions allows one to understand how most people percieve something. This is especially important as knowing multiple opinions can allow someone to take those views into account and sway themseleves to the general audience. Knowing different people's opinion can be beneficial in a variety of situations.\n",
        "\n",
        "First and foremost, a great example about how knowing other's opinions is helpful is when someone is making the choice between smoking or refraining from smoking. A student can watch on a TV channel that smoking is bad, and can damage their internal organs. However, on another channel, the student can find advertisements about the most addicting smoking device that can release the most dopomine in the brain, all the while not severly harming people's lungs. This student will receive a variety of different views and opinions on a certain topic, which allows them to make the best educated choice or decision based on how they interpret what they saw. Similarily, a student can be told from his fellow classmates that smoking is fun, joyful, and makes them happy. However, if the student asks a local doctor, they will be informed differently. A doctor will most likely tell them that smoking, although seeming harmless at first, can lead to serious long term consequences.\n",
        "\n",
        "If the student asks both his friends and his doctors, he is able to use his judgemental skills to determing which choice will be best for him in the long run.\n",
        "\n",
        "Furthermore, asking for multiple opinions can benifit during competitions for a position slot, as cadidates needs to make decisions on what they need to say or do. For example, it can be helpful in situations like elections, both for the U.S. or simply in school. If a student is running for a position in office to represent his/her school, he/she can ask a widespread and diverse audience. First, asking other students is their best bet to obtaining information. Other students can inform him/her about what they want, like better water fountains, recess, or healthier food. Then, the student running can make changes to the way they run for the election, and on his/her speech, take a different approach. In addition, if the student running asks an adult, they will get to know a more realistic way the school can be improved. Since a student, even as a student officer, isn't able to make a significant change to a school, they can inform the school board about ways to make the school better. If someone is running for the president of the United States, a similar approach can be taken. First, they can ask the people, on social media or in speeches, about positive ways to reform our country. After the candidate receives the opinion of general audiences, they can campaign differently to match the view of those voting. All in all, asking for the opinion of multiple different people can set the candidate apart from others.\n",
        "\n",
        "Many people only ask one type of audience for their opinion. Having only one opinion can lead to negative consequences, such as making the wrong choices related to health or education, as only one audience is adressed into making a decision. Therefore, asking multiple different people who have different backgrounds is essential to making the best choices in life. Conclusively, knowing multiple opinions on a certain matter can evidently lead to better results for individuals. \n",
        "\"\"\"\n",
        "preds = infer(test_essay)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'preds' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 64\u001b[0m\n\u001b[1;32m     59\u001b[0m     html_code \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m</p>\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m html_code\n\u001b[0;32m---> 64\u001b[0m segments \u001b[38;5;241m=\u001b[39m [ceil(x\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[43mpreds\u001b[49m]\n\u001b[1;32m     66\u001b[0m html_code \u001b[38;5;241m=\u001b[39m generate_html_with_highlight(test_essay, segments)\n\u001b[1;32m     67\u001b[0m HTML(html_code)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'preds' is not defined"
          ]
        }
      ],
      "source": [
        "from IPython.display import HTML\n",
        "from math import ceil\n",
        "import numpy as np\n",
        "\n",
        "segment_colors = {\n",
        "  0: 'rgba(0,0,0, 0.0)',\n",
        "  1: 'rgba(206,95,20, 0.8)',\n",
        "  2: 'rgba(114,174,146, 0.8)',\n",
        "  3: 'rgba(251,174,28, 0.8)',\n",
        "  4: 'rgba(81,53,51, 0.8)',\n",
        "  5: 'rgba(43,112,133, 0.8)',\n",
        "  6: 'rgba(200,109,142, 0.8)',\n",
        "  7: 'rgba(243,218,179, 0.7)',\n",
        "}\n",
        "\n",
        "segment_names = {\n",
        "  0: 'Unnanotated',\n",
        "  1: 'Lead',\n",
        "  2: 'Position',\n",
        "  3: 'Evidence',\n",
        "  4: 'Claim',\n",
        "  5: 'Concluding Statement',\n",
        "  6: 'Counterclaim',\n",
        "  7: 'Rebuttal'\n",
        "}\n",
        "\n",
        "def generate_html_with_highlight(original_text, segment_types):\n",
        "\n",
        "    # Hover stuff\n",
        "    html_code = '<style>'\n",
        "    html_code += '.segment-highlight:hover:before { content: attr(data-label); position: absolute; background: #111; color: #fff; padding: 4px 8px; border-radius: 4px; z-index: 2; font-size: 14px}'\n",
        "    html_code += '.segment-highlight:hover:after { content: attr(data-label); position: absolute; background: #111; color: #fff; padding: 4px 8px; border-radius: 4px; z-index: 2; font-size: 14px}'\n",
        "    html_code += '</style>'\n",
        "\n",
        "    # Generate a legend showing what segment type each color represents\n",
        "    html_code += '<p style=\"font-size: 18px; line-height: 1.6;\"><b>Legend:</b><br/>'\n",
        "    for segment_color, segment_name in zip(segment_colors.values(), segment_names.values()):\n",
        "        html_code += f'<span style=\"background-color: {segment_color};\">{segment_name}</span><br/>'\n",
        "    html_code += '<br/></p>'\n",
        "\n",
        "    html_code += '<p style=\"font-size: 18px; line-height: 1.6;\">You can also hover on top of each segment to see their type<br/><br/></p>'\n",
        "\n",
        "    # Highlight original text\n",
        "    html_code += '<p style=\"font-size: 18px; line-height: 1.6; background-color: white; color: black;\"><b>Segmented Essay:</b><br/>'\n",
        "    current_segment_type = None\n",
        "    for i, (word, segment_type) in enumerate(zip(original_text.split(), segment_types)):\n",
        "        if i > 0 and segment_type != segment_types[i - 1]:\n",
        "            html_code += f'</span>'\n",
        "\n",
        "        if segment_type != current_segment_type:\n",
        "            html_code += f'<span class=\"segment-highlight\" style=\"background-color: {segment_colors[segment_type]};\" data-label=\"{segment_names[segment_type]}\">'\n",
        "            current_segment_type = segment_type\n",
        "\n",
        "        html_code += f'{word} '\n",
        "\n",
        "    if current_segment_type is not None:\n",
        "        html_code += '</span>'\n",
        "\n",
        "    html_code += '</p>'\n",
        "\n",
        "    return html_code\n",
        "\n",
        "\n",
        "segments = [ceil(x/2) for x in preds]\n",
        "\n",
        "html_code = generate_html_with_highlight(test_essay, segments)\n",
        "HTML(html_code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
