{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuUEjAwP1OQc"
      },
      "source": [
        "## Imports and configs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ITOAckJ7RJYC"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, AutoConfig\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lx_rAHzX2bmq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training on cuda\n"
          ]
        }
      ],
      "source": [
        "config = {'model_name': 'google/bigbird-roberta-base', # From Huggingface's ModelHub.\n",
        "          'model_save_path': './model/',\n",
        "          'model_chkpt_path': './model/model_chkpt/',\n",
        "          'max_length': 1024,\n",
        "          'train_batch_size': 4,\n",
        "          'valid_batch_size': 4,\n",
        "          'epochs':5,\n",
        "          'learning_rates': [2.5e-5, 2.5e-5, 2.5e-6, 2.5e-6, 2.5e-7],\n",
        "          'max_grad_norm': 10,\n",
        "          'device': 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'} # Added support for Apple Metal acceleration.\n",
        "\n",
        "print(f\"Training on {config['device']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "bxk_MhBgwk5x",
        "outputId": "cd65b40d-8d07-4ef9-ed98-6f97ee280cba"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>73DC1D49FAD5</th>\n",
              "      <td>eletoral college can be a very good thing caus...</td>\n",
              "      <td>[3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>D840AC3957E5</th>\n",
              "      <td>STUDENT_NAME\\n\\nADDRESS_NAME\\n\\nFebruary 22, 2...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 3, 4, 4, 4, 4, 4, 4, 4, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>753E320B186B</th>\n",
              "      <td>In my opinion as a student: I don't agree at t...</td>\n",
              "      <td>[1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C2ABDAC2BC2C</th>\n",
              "      <td>When it comes to at home learning and attendin...</td>\n",
              "      <td>[3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>B2DDBAAC084C</th>\n",
              "      <td>Y\\n\\nou can ask many different people for advi...</td>\n",
              "      <td>[3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                        content  \\\n",
              "id                                                                \n",
              "73DC1D49FAD5  eletoral college can be a very good thing caus...   \n",
              "D840AC3957E5  STUDENT_NAME\\n\\nADDRESS_NAME\\n\\nFebruary 22, 2...   \n",
              "753E320B186B  In my opinion as a student: I don't agree at t...   \n",
              "C2ABDAC2BC2C  When it comes to at home learning and attendin...   \n",
              "B2DDBAAC084C  Y\\n\\nou can ask many different people for advi...   \n",
              "\n",
              "                                                         labels  \n",
              "id                                                               \n",
              "73DC1D49FAD5  [3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...  \n",
              "D840AC3957E5  [0, 0, 0, 0, 0, 0, 0, 3, 4, 4, 4, 4, 4, 4, 4, ...  \n",
              "753E320B186B  [1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...  \n",
              "C2ABDAC2BC2C  [3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...  \n",
              "B2DDBAAC084C  [3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "full_df = pd.read_pickle('../dataset.csv')\n",
        "full_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Labels\n",
        "\n",
        "Because we have 15k+ essays, each with hundreds of labeled words, we store the labels as integers rather than strings to drastically decrease disk and memory usage (see `preprocessing.ipynb` for how this was done). Additionally, we would have needed to do this for training regardless to build the one-hot arrays. These can be easily converted to/from their original string labels with the dictionaries below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "sKy9ejb77YQ6"
      },
      "outputs": [],
      "source": [
        "id2label = {\n",
        "  0:  'Unnanotated',\n",
        "  1:  'B-Lead',\n",
        "  2:  'I-Lead',\n",
        "  3:  'B-Position',\n",
        "  4:  'I-Position',\n",
        "  5:  'B-Evidence',\n",
        "  6:  'I-Evidence',\n",
        "  7:  'B-Claim',\n",
        "  8:  'I-Claim',\n",
        "  9:  'B-Concluding_Statement',\n",
        "  10: 'I-Concluding_Statement',\n",
        "  11: 'B-Counterclaim',\n",
        "  12: 'I-Counterclaim',\n",
        "  13: 'B-Rebuttal',\n",
        "  14: 'I-Rebuttal'\n",
        "}\n",
        "\n",
        "label2id = {\n",
        "  'Unnanotated': 0,\n",
        "  'B-Lead': 1,\n",
        "  'I-Lead': 2,\n",
        "  'B-Position': 3,\n",
        "  'I-Position': 4,\n",
        "  'B-Evidence': 5,\n",
        "  'I-Evidence': 6,\n",
        "  'B-Claim': 7,\n",
        "  'I-Claim': 8,\n",
        "  'B-Concluding_Statement': 9,\n",
        "  'I-Concluding_Statement': 10,\n",
        "  'B-Counterclaim': 11,\n",
        "  'I-Counterclaim': 12,\n",
        "  'B-Rebuttal': 13,\n",
        "  'I-Rebuttal': 14\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset\n",
        "\n",
        "Since we're using the PyTorch backend, it is convenient to define the torch Dataset (and later the Dataloader), so that the model can easily intake the data without any dependencies on how the we chose to store the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "SjLfEutKxepV"
      },
      "outputs": [],
      "source": [
        "class EssayDataset(Dataset):\n",
        "  def __init__(self, df, tokenizer, max_len, get_word_ids):\n",
        "        self.len = len(df)\n",
        "        self.df = df\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.get_word_ids = get_word_ids # For validation.\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "        essay_words = self.df.content[index].split() # Split essay by words before tokenizing.\n",
        "\n",
        "        # Makes a dict with keys: input_ids, attention_mask.\n",
        "        encoding = self.tokenizer(essay_words,\n",
        "                             is_split_into_words=True, # Necessary to keep correspondance between words and labels contructed previously.\n",
        "                             padding='max_length',\n",
        "                             truncation=True,\n",
        "                             max_length=self.max_len)\n",
        "        \"\"\"\n",
        "        From Tokenizer's docs about word_ids:\n",
        "            A list indicating the word corresponding to each token. Special tokens added by the tokenizer are mapped to None and other tokens\n",
        "            are mapped to the index of their corresponding word (several tokens will be mapped to the same word index if they are parts of that word).\n",
        "\n",
        "        This is needed to match the correct labels with the tokens, which may not have a 1:1 correspondence with the original words.\n",
        "        \"\"\"\n",
        "        word_ids = encoding.word_ids()\n",
        "        word_labels = None\n",
        "\n",
        "        # If we're training, we want to know the labels corresponding to each word_id.\n",
        "        if not self.get_word_ids:\n",
        "            # Get original word label array for this essay.\n",
        "            word_labels = self.df.labels[index] #[int(label) for label in self.df.labels[index].split()]\n",
        "            label_ids = []\n",
        "\n",
        "            # Correct for tokenization mismatch.\n",
        "            for word_idx in word_ids:\n",
        "                # 'None' means that this is a special/reserved token, mark as -100 to be ignored later in training.\n",
        "                if word_idx is None:\n",
        "                    label_ids.append(-100) # Magic number, automatically ignored by CrossEntropyLoss.\n",
        "                else:\n",
        "                    label_ids.append(word_labels[word_idx])\n",
        "\n",
        "            encoding['labels'] = label_ids\n",
        "\n",
        "        # Otherwise, it does not matter since we are predicting the labels, and we jus need to know the token-id correspondence for label attribution.\n",
        "        else:\n",
        "            word_ids2 = [w if w is not None else -1 for w in word_ids]\n",
        "            encoding['word_ids'] = torch.as_tensor(word_ids2)\n",
        "\n",
        "        item = {k: torch.as_tensor(v) for k, v in encoding.items()}\n",
        "\n",
        "        # if self.get_word_ids:\n",
        "        #     word_ids2 = [w if w is not None else -1 for w in word_ids]\n",
        "        #     item['word_ids'] = torch.as_tensor(word_ids2)\n",
        "\n",
        "        return item\n",
        "\n",
        "  def __len__(self):\n",
        "        return self.len"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOJU-81Jv2xO"
      },
      "source": [
        "### Create training and validation datasets + DataLoaders\n",
        "\n",
        "We will use a 85%/15% split between training and validation sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "b4VZ2KiU0AHt"
      },
      "outputs": [],
      "source": [
        "validation_split_size = 0.15\n",
        "dataset_split_seed = 33 # Any seed, here for inter-run consistency."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNmagNKo1E9H"
      },
      "source": [
        "#### Split dataset into training and validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n82RG8PQv8Ph",
        "outputId": "c5fdbbea-2809-42b4-e4cd-8798fc651deb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train_df shape:  (13254, 2)\n",
            "val_df shape:  (2340, 2)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_df, val_df = train_test_split(full_df, test_size=validation_split_size, random_state=dataset_split_seed)\n",
        "\n",
        "# Drop id column from train dataframe\n",
        "train_df = train_df[['content', 'labels']]\n",
        "\n",
        "train_df.reset_index(drop=True, inplace=True)\n",
        "val_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "print(\"train_df shape: \", train_df.shape)\n",
        "print(\"val_df shape: \", val_df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKYQoL-A18pu"
      },
      "source": [
        "#### Download/cache all models necessary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "N1XQ6jVOG0iW"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BigBirdForTokenClassification were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "if not os.path.exists(config['model_save_path']):\n",
        "    os.mkdir(config['model_save_path'])\n",
        "\n",
        "AutoTokenizer.from_pretrained(config['model_name'], add_prefix_space=True, id2label=id2label).save_pretrained(config['model_save_path'])\n",
        "\n",
        "config_model = AutoConfig.from_pretrained(config['model_name']) \n",
        "config_model.num_labels = len(label2id)\n",
        "config_model.save_pretrained(config['model_save_path'])\n",
        "\n",
        "AutoModelForTokenClassification.from_pretrained(config['model_name'], \n",
        "                                                           config=config_model).save_pretrained(config['model_save_path'])\n",
        "\n",
        "del config_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Initialize Dataset, Dataloader, and tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "IgaOgMmW2J46"
      },
      "outputs": [],
      "source": [
        "train_params = {'batch_size': config['train_batch_size'],\n",
        "                'shuffle': True,\n",
        "                # 'num_workers': 2,\n",
        "                }\n",
        "\n",
        "validation_params = {'batch_size': config['valid_batch_size'],\n",
        "               'shuffle': False,\n",
        "            #    'num_workers': 2,\n",
        "               }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxxF_cKHtC70",
        "outputId": "05bcee71-780e-4eca-9b56-5f12a75eabb0"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(config['model_save_path'])\n",
        "\n",
        "training_set = EssayDataset(df=train_df, tokenizer=tokenizer, max_len=config['max_length'], get_word_ids=False)\n",
        "training_loader = DataLoader(training_set, **train_params)\n",
        "\n",
        "testing_set = EssayDataset(df=val_df, tokenizer=tokenizer, max_len=config['max_length'], get_word_ids=True)\n",
        "validation_loader = DataLoader(testing_set, **validation_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([-100,    1,    2,  ..., -100, -100, -100])"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "training_set[0]['labels'].view(-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2ieuYIrIDSO"
      },
      "source": [
        "Save DataLoaders to be able to quickly load them later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "GEcuk2cjHMCV"
      },
      "outputs": [],
      "source": [
        "# torch.save(training_loader, 'training_loader.pth')\n",
        "# torch.save(validation_loader, 'validation_loader.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Initialize all necesary models/objects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        }
      ],
      "source": [
        "config_model = AutoConfig.from_pretrained(f\"{config['model_save_path']}config.json\") \n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "                   f\"{config['model_save_path']}model.safetensors\",config=config_model).to(config['device'])\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=config['learning_rates'][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def train_one_epoch(epoch, verbose=False):\n",
        "    # Since we're batching, and the last batch may not be the full size,\n",
        "    # keep track of precise # steps and # seen examples for metrics computations.\n",
        "    epoch_loss = 0\n",
        "    epoch_accuracy = 0\n",
        "    num_train_examples = 0\n",
        "    num_train_steps = 0\n",
        "    \n",
        "    # Set model to training mode (torch).\n",
        "    model.train()\n",
        "    \n",
        "    for idx, batch in tqdm(enumerate(training_loader), total=len(training_loader)): # TODO: Customize tqdm for better displaying.\n",
        "        \n",
        "        # The dictionary returned by trainin_loader has 3 keys: input_ids, attention_mask (both made by the Tokenizer),\n",
        "        # and labels (which we add in the Dataset abstraction, since this is a training run).\n",
        "        input_ids = batch['input_ids'].to(config['device'], dtype=torch.long)\n",
        "        attention_mask = batch['attention_mask'].to(config['device'], dtype=torch.long)\n",
        "        labels = batch['labels'].to(config['device'], dtype=torch.long)\n",
        "\n",
        "        # Run batch through model.\n",
        "        loss, train_logits = model(input_ids=input_ids,\n",
        "                                   attention_mask=attention_mask,\n",
        "                                   labels=labels,\n",
        "                                   return_dict=False)\n",
        "        \n",
        "        # Increment epoch loss by this batch's loss.\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        # Increment counters.\n",
        "        num_train_steps += 1\n",
        "        num_train_examples += labels.size(0)\n",
        "        \n",
        "        # Debugging.\n",
        "        if verbose and (idx % 100 == 0):\n",
        "            print(f\"Idx: {idx:04d}, step loss: {epoch_loss/num_train_steps}\")\n",
        "           \n",
        "        # Compute training accuracy\n",
        "        flattened_logits = train_logits.view(-1, model.num_labels) # (batch_size, sequence_length, num_labels) -> (batch_size * seq_length, num_labels)\n",
        "        flattened_predictions = torch.argmax(flattened_logits, axis=1) # Find predicted label.\n",
        "        label_mask = labels.view(-1) != -100 # If our label is -100 (as sdiscussed above), it should be ignored as it is a special token.\n",
        "        \n",
        "        # Mask both predictions and ground truth.\n",
        "        labels = torch.masked_select(input=labels.view(-1), mask=label_mask)\n",
        "        predictions = torch.masked_select(input=flattened_predictions, mask=label_mask)\n",
        "\n",
        "        # Use accuracy function from sklear.\n",
        "        epoch_accuracy += accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n",
        "    \n",
        "        # Gradient clipping.\n",
        "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=config['max_grad_norm'])\n",
        "        \n",
        "        # Finally, optimize.\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Model and dataset is pretty big, so alleviate some GPU memory usage by releasing unused cached data at every batch.\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # Normalize loss and accuracy by num steps.\n",
        "    epoch_loss /= num_train_steps\n",
        "    epoch_accuracy /= num_train_steps\n",
        "\n",
        "    print(f\"Training loss epoch: {epoch_loss}\")\n",
        "    print(f\"Training accuracy epoch: {epoch_accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Finally, train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3314/3314 [36:59<00:00,  1.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss epoch: 0.7546362035790336\n",
            "Training accuracy epoch: 0.7529057776726666\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3314/3314 [36:53<00:00,  1.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss epoch: 0.5935034313580324\n",
            "Training accuracy epoch: 0.7960994502283752\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3314/3314 [36:56<00:00,  1.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss epoch: 0.5060606728508555\n",
            "Training accuracy epoch: 0.824042095085337\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3314/3314 [36:54<00:00,  1.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss epoch: 0.4273301211821436\n",
            "Training accuracy epoch: 0.8491220668471002\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3314/3314 [36:56<00:00,  1.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss epoch: 0.3547257536580459\n",
            "Training accuracy epoch: 0.8746208288345277\n"
          ]
        }
      ],
      "source": [
        "if not os.path.exists(config['model_chkpt_path']):\n",
        "    os.mkdir(config['model_chkpt_path'])\n",
        "\n",
        "for epoch in range(config['epochs']):\n",
        "    train_one_epoch(epoch)\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    torch.save(model.state_dict(), f\"{config['model_chkpt_path']}{config['model_name'].replace('/', '-')}-maxlen={config['max_length']}-batchsize={config['train_batch_size']}-lr={str(config['learning_rates'])}-maxgrad={config['max_grad_norm']}-epoch={epoch}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
