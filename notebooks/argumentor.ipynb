{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "rj7JRJCck99l",
        "r11kKYyXl7PY"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Interactive Tool For ArguMentor\n"
      ],
      "metadata": {
        "id": "KcDbw4dBS91B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "\n",
        "*Note*: The model files are pretty large, so downloading and extracting them usually takes a few of minutes."
      ],
      "metadata": {
        "id": "kn4pdqXtuDhX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUUTlgF4uGOR",
        "outputId": "b1ef77ef-cb50-4bda-e891-9d0fb560985b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O model.tar.xz https://uofi.box.com/shared/static/r0uerujvsq2z25odnmtga5gcdlrs1pd4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6rB_r7Wu8g0",
        "outputId": "d46e5782-bbad-47f2-ddb7-30ff9742d816"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-12-15 18:56:17--  https://uofi.box.com/shared/static/r0uerujvsq2z25odnmtga5gcdlrs1pd4\n",
            "Resolving uofi.box.com (uofi.box.com)... 74.112.186.144\n",
            "Connecting to uofi.box.com (uofi.box.com)|74.112.186.144|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /public/static/r0uerujvsq2z25odnmtga5gcdlrs1pd4 [following]\n",
            "--2023-12-15 18:56:17--  https://uofi.box.com/public/static/r0uerujvsq2z25odnmtga5gcdlrs1pd4\n",
            "Reusing existing connection to uofi.box.com:443.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://uofi.app.box.com/public/static/r0uerujvsq2z25odnmtga5gcdlrs1pd4 [following]\n",
            "--2023-12-15 18:56:17--  https://uofi.app.box.com/public/static/r0uerujvsq2z25odnmtga5gcdlrs1pd4\n",
            "Resolving uofi.app.box.com (uofi.app.box.com)... 74.112.186.144\n",
            "Connecting to uofi.app.box.com (uofi.app.box.com)|74.112.186.144|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://public.boxcloud.com/d/1/b1!O9LrI4yku-O7WBzzwXQNVlhEjQyohboutDNuqYixVawJvcQmFhV2tqJXTd4yTQz6GnRPfZNwsUyln_uo3L7qR26AOd4T2brxEg3gU4foX3W6h7EVbat9g4x1SNuV7DdJgQM58l-m96D6oqueyWLX7coppKhUjzVL7mY2GgxR04o_vB-DNrDMV60lts9M1j50mEywUClODNNse57-Audvnf0G-mTld9PlNPuBXfY_-oCgdrubLBXl6vGf7k68XvfTATeEUOdXC2lzC5XrejKLMonBaYXzr0T26eF24Efcy66aasK-_MKHK83fD1DcdawfdpPo6IgLZvHHvAnkfJqEUASask2LJ51E2miPT3vBRwXFESIGwBd_WfVEhakqtZN68HnkYm2HcXEmR-UU1jdnjLZpkUeO1P4BehM27Bk6L-nEWLIMW3y3qA4T4_4IUzJ0eJcbaJ9RnE1o4SWtGhGNrJremyXKyMmGa47Trs9Z8s7oP8pqqY4P8rzU7iEo5FR5bdJr8MSvwdc00htW6zmFDpviJd9cEk-yvXzgppM_s_ePbGQgBlGDZFZqy3zLW7sEofj4mIX9elsSN-5HMzyxuFZB2oe3k3U4IT4EN3E9-sDlEOrNBTClvp9TGttVyvOavBkWqpA-fyuZrSlcSlQqrpqc4toBdmGolmLhqa7I6xamS2bAVaewF18p3oODrvzgV5VJ0zAFnBDij20wwYFClFVslNrd-EsvKCBMXu1Gk0baBTBwAW15n2DJabhC73r9GuWrby1aNd6ticROQlQsGAsuWo0hroK4OUsc3dgBLE4WdSnEUWOsSY7N2lUbgpHR5LC76rSrr63jjfUxYovDaMr8PD7Tyqzz-iasC2TwEJlibWMRCYJ11Cn5Jn32utYzIIC10A33hobI2JIbZaT2PlIOpPtt5aICyDxvlmTILrMHWEgd9zJkgsYPHkdA_tOI-HIztnjcYrxnEZBt1gnVCcbwCRoD-ClNsOH2yfH8LqWBqi4aZ6fELg21rHU-4KXCbNRb-XeZeeoz17D90WqkgaWdLHSpPE6k-KyxWT7-8mqo4eo5yFf4kkAxu4mB94TTuNepHreQpneK-n9XIaRtMrvpl61X45XPjwiFoASG12cZHoRX-0UL3ntavA5geI_6qamxC5DDD-jfwrcuwnB14rHXVqrTjqv3rl-IgWcQPZjqpO90WMoaSUCThSMifpyWqWKn3jhFUra_PUTgquuqXu1nZ1BsY8k0wtEbP0eQgTLCgmP8REszwQnqridHcRswcRSoyBEEcDVdhrR8ouXNEX86UhiQoszKdP1FjPjzUHRO4ffBPSyTSPm5dRDLEPixa56FBVKDzj_ww62KDMF7vPu9DrR6k6jnVKlAzMwlAU65W6Dv_r1tE7c./download [following]\n",
            "--2023-12-15 18:56:18--  https://public.boxcloud.com/d/1/b1!O9LrI4yku-O7WBzzwXQNVlhEjQyohboutDNuqYixVawJvcQmFhV2tqJXTd4yTQz6GnRPfZNwsUyln_uo3L7qR26AOd4T2brxEg3gU4foX3W6h7EVbat9g4x1SNuV7DdJgQM58l-m96D6oqueyWLX7coppKhUjzVL7mY2GgxR04o_vB-DNrDMV60lts9M1j50mEywUClODNNse57-Audvnf0G-mTld9PlNPuBXfY_-oCgdrubLBXl6vGf7k68XvfTATeEUOdXC2lzC5XrejKLMonBaYXzr0T26eF24Efcy66aasK-_MKHK83fD1DcdawfdpPo6IgLZvHHvAnkfJqEUASask2LJ51E2miPT3vBRwXFESIGwBd_WfVEhakqtZN68HnkYm2HcXEmR-UU1jdnjLZpkUeO1P4BehM27Bk6L-nEWLIMW3y3qA4T4_4IUzJ0eJcbaJ9RnE1o4SWtGhGNrJremyXKyMmGa47Trs9Z8s7oP8pqqY4P8rzU7iEo5FR5bdJr8MSvwdc00htW6zmFDpviJd9cEk-yvXzgppM_s_ePbGQgBlGDZFZqy3zLW7sEofj4mIX9elsSN-5HMzyxuFZB2oe3k3U4IT4EN3E9-sDlEOrNBTClvp9TGttVyvOavBkWqpA-fyuZrSlcSlQqrpqc4toBdmGolmLhqa7I6xamS2bAVaewF18p3oODrvzgV5VJ0zAFnBDij20wwYFClFVslNrd-EsvKCBMXu1Gk0baBTBwAW15n2DJabhC73r9GuWrby1aNd6ticROQlQsGAsuWo0hroK4OUsc3dgBLE4WdSnEUWOsSY7N2lUbgpHR5LC76rSrr63jjfUxYovDaMr8PD7Tyqzz-iasC2TwEJlibWMRCYJ11Cn5Jn32utYzIIC10A33hobI2JIbZaT2PlIOpPtt5aICyDxvlmTILrMHWEgd9zJkgsYPHkdA_tOI-HIztnjcYrxnEZBt1gnVCcbwCRoD-ClNsOH2yfH8LqWBqi4aZ6fELg21rHU-4KXCbNRb-XeZeeoz17D90WqkgaWdLHSpPE6k-KyxWT7-8mqo4eo5yFf4kkAxu4mB94TTuNepHreQpneK-n9XIaRtMrvpl61X45XPjwiFoASG12cZHoRX-0UL3ntavA5geI_6qamxC5DDD-jfwrcuwnB14rHXVqrTjqv3rl-IgWcQPZjqpO90WMoaSUCThSMifpyWqWKn3jhFUra_PUTgquuqXu1nZ1BsY8k0wtEbP0eQgTLCgmP8REszwQnqridHcRswcRSoyBEEcDVdhrR8ouXNEX86UhiQoszKdP1FjPjzUHRO4ffBPSyTSPm5dRDLEPixa56FBVKDzj_ww62KDMF7vPu9DrR6k6jnVKlAzMwlAU65W6Dv_r1tE7c./download\n",
            "Resolving public.boxcloud.com (public.boxcloud.com)... 74.112.186.128\n",
            "Connecting to public.boxcloud.com (public.boxcloud.com)|74.112.186.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 469470544 (448M) [application/octet-stream]\n",
            "Saving to: ‘model.tar.xz’\n",
            "\n",
            "model.tar.xz        100%[===================>] 447.72M  42.1MB/s    in 11s     \n",
            "\n",
            "2023-12-15 18:56:30 (39.7 MB/s) - ‘model.tar.xz’ saved [469470544/469470544]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar xf model.tar.xz"
      ],
      "metadata": {
        "id": "zTP8x7ydyQEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import and Model Loading"
      ],
      "metadata": {
        "id": "rj7JRJCck99l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "import torch"
      ],
      "metadata": {
        "id": "oyJhXrpCa8tL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {'model_name': 'google/bigbird-roberta-base', # From Huggingface's ModelHub.\n",
        "          'model_save_path': './model/',\n",
        "          'max_length': 1024,\n",
        "          'device': 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'}\n",
        "print(f\"Infering on {config['device']}\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(config['model_name'])\n",
        "model = AutoModelForTokenClassification.from_pretrained(config['model_save_path']).to(config['device'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GzL4k-P9v8yR",
        "outputId": "79524255-8938-4b3e-dba7-e4e8faf495fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Infering on cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def infer(essay):\n",
        "    # Makes a dict with keys: input_ids, attention_mask.\n",
        "    encoding = tokenizer(essay.split(),\n",
        "                         is_split_into_words=True, # Necessary to keep correspondance between words and labels contructed previously.\n",
        "                         padding='max_length',\n",
        "                         truncation=True,\n",
        "                         max_length=config['max_length'])\n",
        "    \"\"\"\n",
        "    From Tokenizer's docs about word_ids:\n",
        "        A list indicating the word corresponding to each token. Special tokens added by the tokenizer are mapped to None and other tokens\n",
        "        are mapped to the index of their corresponding word (several tokens will be mapped to the same word index if they are parts of that word).\n",
        "\n",
        "    This is needed to match the correct labels with the tokens, which may not have a 1:1 correspondence with the original words.\n",
        "    \"\"\"\n",
        "    encoding['word_ids'] = torch.as_tensor([w if w is not None else -1 for w in encoding.word_ids()])\n",
        "\n",
        "    item = {k: torch.unflatten(torch.as_tensor(v), 0, (1, -1)).to(config['device']) for k, v in encoding.items()}\n",
        "\n",
        "    model.eval()\n",
        "    output_dict = model(input_ids=item['input_ids'],\n",
        "                   attention_mask=item['attention_mask'],\n",
        "                   return_dict=True)\n",
        "\n",
        "    token_predictions = torch.argmax(output_dict['logits'].view(-1, model.num_labels), axis=-1)\n",
        "\n",
        "    words_predictions = list()\n",
        "    prev_word_idx = -1\n",
        "    for idx, word_idx in enumerate(item['word_ids'][0]):\n",
        "        if word_idx == -1:\n",
        "            continue\n",
        "        elif word_idx != prev_word_idx:\n",
        "            prev_word_idx = word_idx\n",
        "            words_predictions.append(token_predictions[idx].item())\n",
        "\n",
        "    return words_predictions"
      ],
      "metadata": {
        "id": "IIkfvLQeuMh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper Functions"
      ],
      "metadata": {
        "id": "r11kKYyXl7PY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "segment_colors = {\n",
        "  0: 'rgba(0,0,0, 0.0)',\n",
        "  1: 'rgba(206,95,20, 0.8)',\n",
        "  2: 'rgba(114,174,146, 0.8)',\n",
        "  3: 'rgba(251,174,28, 0.8)',\n",
        "  4: 'rgba(81,53,51, 0.8)',\n",
        "  5: 'rgba(43,112,133, 0.8)',\n",
        "  6: 'rgba(200,109,142, 0.8)',\n",
        "  7: 'rgba(243,218,179, 0.7)',\n",
        "}\n",
        "\n",
        "segment_names = {\n",
        "  0: 'Unnanotated',\n",
        "  1: 'Lead',\n",
        "  2: 'Position',\n",
        "  3: 'Evidence',\n",
        "  4: 'Claim',\n",
        "  5: 'Concluding Statement',\n",
        "  6: 'Counterclaim',\n",
        "  7: 'Rebuttal'\n",
        "}"
      ],
      "metadata": {
        "id": "FsSBsGBunaEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_html_with_highlight(original_text, segment_types):\n",
        "\n",
        "    # Hover stuff\n",
        "    html_code = '<style>'\n",
        "    html_code += '.segment-highlight:hover:before { content: attr(data-label); background: #111; color: #fff; padding: 4px 8px; border-radius: 4px; z-index: 2; font-size: 14px}'\n",
        "    html_code += '.segment-highlight:hover:after { content: attr(data-label); background: #111; color: #fff; padding: 4px 8px; border-radius: 4px; z-index: 2; font-size: 14px}'\n",
        "    html_code += '</style>'\n",
        "\n",
        "    # Generate a legend showing what segment type each color represents\n",
        "    html_code += '<p style=\"font-size: 18px; line-height: 1.6;\"><b>Legend:</b><br/>'\n",
        "    for segment_color, segment_name in zip(segment_colors.values(), segment_names.values()):\n",
        "        html_code += f'<span style=\"background-color: {segment_color};\">{segment_name}</span><br/>'\n",
        "    html_code += '<br/></p>'\n",
        "\n",
        "    html_code += '<p style=\"font-size: 18px; line-height: 1.6;\">You can also hover on top of each segment to see their type<br/><br/></p>'\n",
        "\n",
        "    # Highlight original text\n",
        "    html_code += '<p style=\"font-size: 18px; line-height: 1.6; background-color: white; color: black;\"><b>Segmented Essay:</b><br/>'\n",
        "    current_segment_type = None\n",
        "    for i, (word, segment_type) in enumerate(zip(original_text.split(), segment_types)):\n",
        "        if i > 0 and segment_type != segment_types[i - 1]:\n",
        "            html_code += f'</span>'\n",
        "\n",
        "        if segment_type != current_segment_type:\n",
        "            html_code += f'<span class=\"segment-highlight\" style=\"background-color: {segment_colors[segment_type]};\" data-label=\"{segment_names[segment_type]}\">'\n",
        "            current_segment_type = segment_type\n",
        "\n",
        "        html_code += f'{word} '\n",
        "\n",
        "    if current_segment_type is not None:\n",
        "        html_code += '</span>'\n",
        "\n",
        "    html_code += '</p>'\n",
        "\n",
        "    return html_code"
      ],
      "metadata": {
        "id": "18UYluoOaqJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interactive Tool"
      ],
      "metadata": {
        "id": "xgvgNtkVl5Sz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Segment Your Essay\n",
        "from math import ceil\n",
        "\n",
        "essay = \"To overcome Hume\\u2019s problem of induction and derive a theorem that permits learning \\u2013 and, thus, relies on induction working\\u2013, Valiant claims it is necessary to make two assumptions about the world: The Invariance Assumption, and the Learnable Regularity Assumption.  The first \\u2013 the Invariance Assumption \\u2013 assumes that the context in which a certain generalization is used to make predictions cannot be different that in which this generalization was drawn. This should make intuitive sense and, in a similar way to Hume\\u2019s Uniformity Principle, assumes that the universe is consistent and uniform \\u2013 so much so that if the context (or conditions) is the same, it is possible to use previously noticed patters to make predictions about the world with relative confidence. The second \\u2013 the Learnable Regularity Assumption \\u2013 assumes that objects/things of a same class/category have a few regularities which, when observed over a big-enough sample of such objects, allows one to differentiate this object from others, allowing for categorization.  The combination of both these assumptions, then, allows induction to be probably correct and learning to be achievable. From there, then, Valiant derives his theorem that there is a finite number of samples needed to extract such identifying characteristics from a certain category of objects such that the prediction will remain under a certain error margin \\u2013 and thus, probably correct.\" # @param {type:\"string\"}\n",
        "\n",
        "preds = infer(essay)\n",
        "segments = [ceil(pred/2) for pred in preds] # Adjust for B/I tags.\n",
        "\n",
        "\n",
        "html_code = generate_html_with_highlight(essay, segments)\n",
        "HTML(html_code)"
      ],
      "metadata": {
        "id": "eIwv67xVhMkS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 577
        },
        "outputId": "5b103df8-40fc-48d8-b9f1-10d078badccc",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "224 224\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>.segment-highlight:hover:before { content: attr(data-label); background: #111; color: #fff; padding: 4px 8px; border-radius: 4px; z-index: 2; font-size: 14px}.segment-highlight:hover:after { content: attr(data-label); background: #111; color: #fff; padding: 4px 8px; border-radius: 4px; z-index: 2; font-size: 14px}</style><p style=\"font-size: 18px; line-height: 1.6;\"><b>Legend:</b><br/><span style=\"background-color: rgba(0,0,0, 0.0);\">Unnanotated</span><br/><span style=\"background-color: rgba(206,95,20, 0.8);\">Lead</span><br/><span style=\"background-color: rgba(114,174,146, 0.8);\">Position</span><br/><span style=\"background-color: rgba(251,174,28, 0.8);\">Evidence</span><br/><span style=\"background-color: rgba(81,53,51, 0.8);\">Claim</span><br/><span style=\"background-color: rgba(43,112,133, 0.8);\">Concluding Statement</span><br/><span style=\"background-color: rgba(200,109,142, 0.8);\">Counterclaim</span><br/><span style=\"background-color: rgba(243,218,179, 0.7);\">Rebuttal</span><br/><br/></p><p style=\"font-size: 18px; line-height: 1.6;\">You can also hover on top of each segment to see their type<br/><br/></p><p style=\"font-size: 18px; line-height: 1.6; background-color: white; color: black;\"><b>Segmented Essay:</b><br/><span class=\"segment-highlight\" style=\"background-color: rgba(114,174,146, 0.8);\" data-label=\"Position\">To overcome Hume’s problem of induction and derive a theorem that permits learning – and, thus, relies on induction working–, </span><span class=\"segment-highlight\" style=\"background-color: rgba(81,53,51, 0.8);\" data-label=\"Claim\">Valiant claims it is necessary to make two assumptions about the world: The Invariance Assumption, </span><span class=\"segment-highlight\" style=\"background-color: rgba(0,0,0, 0.0);\" data-label=\"Unnanotated\">and </span><span class=\"segment-highlight\" style=\"background-color: rgba(81,53,51, 0.8);\" data-label=\"Claim\">the Learnable Regularity Assumption. The first – the Invariance Assumption – assumes that the context in which a certain generalization is used to make predictions cannot be different that in which this generalization was drawn. </span><span class=\"segment-highlight\" style=\"background-color: rgba(251,174,28, 0.8);\" data-label=\"Evidence\">This should make intuitive sense and, in a similar way to Hume’s Uniformity Principle, assumes that the universe is consistent and uniform – so much so that if the context (or conditions) is the same, it is possible to use previously noticed patters to make predictions about the world with relative confidence. </span><span class=\"segment-highlight\" style=\"background-color: rgba(81,53,51, 0.8);\" data-label=\"Claim\">The second – the Learnable Regularity Assumption – </span><span class=\"segment-highlight\" style=\"background-color: rgba(251,174,28, 0.8);\" data-label=\"Evidence\">assumes that objects/things of a same class/category have a few regularities which, when observed over a big-enough sample of such objects, allows one to differentiate this object from others, allowing for categorization. The combination of both these assumptions, then, allows induction to be probably correct and learning to be achievable. From there, then, Valiant derives his theorem that there is a finite number of samples needed to extract such identifying characteristics from a certain category of objects such that the prediction will remain under a certain error margin – and thus, probably correct. </span></p>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    }
  ]
}